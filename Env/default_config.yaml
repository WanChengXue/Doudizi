env:
  # 并行化环境的数目
  parallel_env: 100
  env_name: "MinitaurBulletEnv-v0"
  # ------- 定义动作空间和状态空间的维度 --------------
  agent_nums: 1
  

policy_name: 'default_policy'
training_mode: Train
# 要不要保存上一次训练得到的历史队列
use_history: fasle
# learning rate 为0的时间,用于优化器的初始化
warmup_time: 0
# ip of log server, config_server
main_server_ip: '10.151.144.47'
# 10.19.93.9
# 定义日志文件的文件夹
log_dir: "./logs"
# 定义日志服务器的端口
log_server_port: 8100
# 这个表示的是将历史的模型保存20个
model_cache_size: 20
# 这两个端口分别表示,worker请求的端口,以及learner将模型通过哪一个端口发布出去
config_server_model_from_learner: 9000
config_server_model_to_worker: 9001
# 主模型更新时间间隔,这个变量主要是worker端使用，没隔20s调用一次fetcher从config server获取一次最新的模型 -------
sampler_model_update_interval: 20
# ----------- configserver打开http的端口 -----------
config_server_http_port: 9002
# ========== learner related ==============
# ----- 这个变量是learner端使用，表示发布新模型的时间间隔 -----
model_update_intervel: 0
# --------- 这个变量是dataserver是从buffer里面采一个batch的数据放入到plasma client里面的时间间隔 -----------------
data_server_sampling_interval: 10
# ------- 将worker端采样得到的数据保存到本地的文件夹 --------
data_saved_folder: 'Data_saved'
policy_config:
  # ---- 主机索引 -----------
  main_machine_index: 0
  machine_list: ['10.151.144.47']
  # 定义一个设备对应与多少个数据server
  server_number_per_device: 2
  # 定义一个机器有多少个设备
  device_number_per_machine: 1
  # -- 这个参数表示的是数据服务开始端口 ----------
  start_data_server_port: 9527
  # ------ 训练类型 ------------
  policy_type: 'latest'
  training_type: 'supervised_learning'
  algorithm: SL
  # Pytorch DDP相关
  ddp_port: 50001
  batch_size: 32
  plasma_server_location: Plasma_server/plasma
  capacity: 50
  # ------- 这两个参数在训练RL的时候会使用到，如果都是False，说明actor和critic在一起 --------
  centralized_critic: &centralized_critic False
  seperate_critic: &seperate_critic False
  # -------- 模型训练相关的参数 ---------
  training_parameters:
    # ===== 梯度计算的时候，最大梯度值 ====
    max_grad_norm: 10 
    # ===== SL训练的时候，L2正则的系数 =====
    regulization: 1e-4
    centralized_critic: *centralized_critic
    seperate_critic: *seperate_critic
  # -------- 每训练1000次就保存一下模型 ----------
  model_save_interval: 1000
  # --------- 每采样128个点发送一次数据 -------------
  woker_transmit_interval: 5
  # ---------- 定义最大采样长度 ----------
  max_trajectory_length: 1000
  # ------ 设置模型的保存路径 ——----------
  # ======= 两个路径一个是最新的模型构成的模型池，另外一个是每训练若干次保存下来的模型 =====
  model_pool_path: "Exp/Model/model_pool/"
  saved_model_path: "Exp/Model/saved_model/"
  tensorboard_folder: "Tensorboard_log"
  # ----- 只有一个智能体，显然是参数共享和同质化都是True ---------
  parameter_sharing: &parameter_sharing True
  homogeneous_agent: &homogeneous_agent True
  # ------ 这个地方采用的cosineannealingwarmrestarts()学习率调整 ------------
  T_zero: 5
  # ------- 智能体相关的参数 ----------
  agent:
    policy:
      default:
        learning_rate: 1e-4
        model_path: "agent_default_policy_net.pkl"
        state_dim: 239
        action_dim: 5
        model_name: 'FC_model'
        model_type: 'policy'


  


