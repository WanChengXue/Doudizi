# 这个是用来执行独立MARL环境
env:
  # 并行化环境的数目
  parallel_env: 100
  env_name: MinitaurBulletEnv-v0
  # ------- 定义动作空间和状态空间的维度 --------------
  agent_nums: 1
  agent_name_list: ['default']

policy_name: 'SAC_exp'
# 定义日志文件的文件夹
log_dir: "./logs"
# ----  这个变量表示要不要从model pool中载入模型 -----
load_data_from_model_pool: True
policy_config:
  training_type: 'RL'
  eval_mode: True
  # ------ 使用categorical critic net -------
  categorical_critic_net: False
  agent_name: 'sac_agent'
  double_dqn: True
  # ---- 这个变量表示要不要使用target网络 ------
  using_target_network: True
  centralized_critic: &centralized_critic False
  seperate_critic: &seperate_critic True
  # --------- 使用n step TD -----
  n_step: &n_step 1
  gamma: &gamma 0.99
  # ------ 设置模型的保存路径 ——----------
  # ======= 两个路径一个是最新的模型构成的模型池，另外一个是每训练若干次保存下来的模型 =====
  pretrained_model_path: "Exp/Model/model_pool/"
  saved_model_path: "Exp/Model/saved_model/"
  tensorboard_folder: "Tensorboard_log"
  # ----- 只有一个智能体，显然是参数共享和同质化都是True ---------
  parameter_sharing: &parameter_sharing False
  training_parameters:
    # ===== 梯度计算的时候，最大梯度值 ====
    max_grad_norm: 10 
    # ===== discount ratio =====
    gamma: *gamma
    # ------- n_step TD -------
    n_step: *n_step
    parameter_sharing: *parameter_sharing
    centralized_critic: *centralized_critic
    seperate_critic: *seperate_critic
    # -------- soft-update parameter ----
    tau: 5e-3
    n_atoms: &n_atoms 51
    value_min: &value_min -10
    value_max: &value_max 10
    entropy_coefficient: 0.4
    action_dim: &action_dim 8
    adaptive_alpha: &adaptive_alpha True
  # ------- 智能体相关的参数 ----------
  agent:
    # --------- 这个agent的key必须要和上面的agent_name list中的相匹配,如果是parameter_sharing的情况,会在config parser中进行修改 -------
    default:
      policy:
        learning_rate: 3e-4
        state_dim: &state_dim 28
        action_dim: *action_dim
        model_name: 'SAC_model'
        model_type: 'policy'
        adaptive_alpha: *adaptive_alpha

      critic:
        learning_rate: 3e-4
        model_name: 'SAC_model'
        model_type: 'critic'
        hidden_dim: 400
        state_dim: *state_dim
        action_dim: *action_dim
        n_atoms: *n_atoms
        value_min: *value_min
        value_max: *value_max

      # -------- 定义探索策略 ------
      action_dim: *action_dim
      action_low: -1
      action_high: 1
      eval_mode: True
  # --------- 如果出现了中心化的critic,优先级和agent并列 ----------



