# 这个是用来执行独立MARL环境
env:
  parallel_env: 100
  env_name: MinitaurBulletEnv-v0
  # ------- 定义动作空间和状态空间的维度 --------------
  agent_nums: 1
  agent_name_list: ['default']
  multiagent_scenario: False
  
policy_name: 'MAPPO_exp'
training_mode: Train
# learning rate 为0的时间,用于优化器的初始化
warmup_time: 0
# ip of log server, config_servers
main_server_ip: '10.19.93.9'
# 10.19.93.9
# 定义日志文件的文件夹
log_dir: "Exp/Log"
# 定义日志服务器的端口
log_server_port: 8100
# 这个表示的是将历史的模型保存20个
model_cache_size: 20
# 这两个端口分别表示,worker请求的端口,以及learner将模型通过哪一个端口发布出去
config_server_model_from_learner: 9000
config_server_model_to_worker: 9001
# 主模型更新时间间隔,这个变量主要是worker端使用，没隔20s调用一次fetcher从config server获取一次最新的模型 -------
sampler_model_update_interval: 20
# ----------- configserver打开http的端口 -----------
config_server_http_port: 9002
# ========== learner related ==============
# ----- 这个变量是learner端使用，表示发布新模型的时间间隔 -----
model_update_intervel: 0
# --------- 这个变量是dataserver是从buffer里面采一个batch的数据放入到plasma client里面的时间间隔 -----------------
data_server_sampling_interval: 0
# ------- 将worker端采样得到的数据保存到本地的文件夹 --------
data_saved_folder: 'Data_saved'
# ----- 要不要开启数据保存 ------
data_save_start: False
# ----  这个变量表示要不要从model pool中载入模型 -----
load_data_from_model_pool: False
policy_config:
  # ---- 主机索引 -----------
  main_machine_index: 0
  machine_list: ['10.19.93.9']
  # 定义一个设备对应与多少个数据server
  server_number_per_device: 2
  # 定义一个机器有多少个设备
  device_number_per_machine: 1
  # -- 这个参数表示的是数据服务开始端口 ----------
  start_data_server_port: 9527
  # ------ 训练类型 ------------
  policy_type: 'latest'
  training_type: 'RL'
  algorithm: MAPPO
  agent_name: 'mappo_agent'
  policy_based_RL: True
  # Pytorch DDP相关
  ddp_port: 50001
  batch_size: &batch_size 256
  plasma_server_location: Plasma_server/plasma
  # ------- 这两个参数在训练RL的时候会使用到，如果都是False，说明actor和critic在一起 --------
  centralized_critic: &centralized_critic False
  # -------- 每训练1000次就保存一下模型 ----------
  model_save_interval: 1000
  # --------- 每采样20个点发送一次数据 -------------
  woker_transmit_interval: 20
  # ---------- 定义最大采样长度 ----------
  max_trajectory_length: 1000
  # --------- 使用n step TD -----
  n_step: &n_step 1
  gamma: &gamma 0.99
  tau: &tau 0.95
  # ------ 设置模型的保存路径 ——----------
  # ======= 两个路径一个是最新的模型构成的模型池，另外一个是每训练若干次保存下来的模型 =====
  model_pool_path: "Exp/Model/model_pool/"
  # ----- 这个路径表示的是预训练的时候，将模型保存的路径 --------
  pretrained_model_path: "Exp/Model/pretrained_model_pool/"
  # ------ 需要一个字典，表示生成预训练模型的算法名称是什么 ------
  imitation_policy_name: 'multi_point_heterogeneous_policy'
  saved_model_path: "Exp/Model/saved_model/"
  tensorboard_folder: "Tensorboard_log"
  # ----- 只有一个智能体，显然是参数共享和同质化都是True ---------
  parameter_sharing: &parameter_sharing False
  # ------ 这个地方采用的cosineannealingwarmrestarts()学习率调整 ------------
  T_zero: 5
  # -------- 定义replay buffer相关的参数 -------
  priority_replay_buffer: False
  replay_buffer_config:
    buffer_name: data_buffer
    capacity: 10000
    batch_size: *batch_size
  # -------- 模型训练相关的参数 ---------
  training_parameters:
    # ===== 梯度计算的时候，最大梯度值 ====
    clip_value: False
    max_grad_norm: 0.5
    clip_epsilon: 0.2
    dual_clip: 2
    entropy_coef: 0.01
    # ===== discount ratio =====
    gamma: *gamma
    # ------- n_step TD -------
    n_step: *n_step
    parameter_sharing: *parameter_sharing
    centralized_critic: *centralized_critic
  # ------- 智能体相关的参数 ----------
  agent:
    default:
      policy:
        learning_rate: 5e-5
        state_dim: &state_dim 28
        action_dim: &action_dim 8
        model_name: 'PPO_model'
        model_type: 'policy'
    
      critic:
        learning_rate: 1e-3
        model_name: 'PPO_model'
        model_type: critic
        state_dim: *state_dim
        action_dim: *action_dim

    # centralized_critic:
    #   learning_rate: 1e-3
    #   model_name: 'PPO_model'
    #   model_type: 'critic'





